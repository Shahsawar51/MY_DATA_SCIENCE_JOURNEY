{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNl+OT9XsgisnscYgOG9IFc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahsawar51/MY_DATA_SCIENCE_JOURNEY/blob/main/_linear_reg_theory(vip).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Problem Statement - Prediction Ki Zaroorat Kyu Padi?**  \n",
        "Hamari duniya mein bohot si jagah pe **prediction** zaroori hoti hai. Jaise:  \n",
        "- **Stock Market** ‚Üí Agle din ka price predict karna.  \n",
        "- **Temperature Forecasting** ‚Üí Agle din ka temperature estimate karna.  \n",
        "- **Sales Prediction** ‚Üí Kisi product ki agle mahine ki sales nikalna.  \n",
        "\n",
        "Lekin problem ye hai ki **past data ko dekh ke future predict kaise karein?** ü§î  \n",
        "Hamein **ek aisi equation chahiye** jo humein koi bhi **X input do, to Y output mil jaye**.  \n",
        "\n",
        "---\n",
        "### **Next Step: X aur Y ke Beech Relation Samajhna**  \n",
        "Tumhe samajhna hoga ki **X ka Y ke sath relation linear hai ya nahi**.  \n",
        "Ab batao, **scatter plot** banayen taake ye check karein? üîç"
      ],
      "metadata": {
        "id": "dCISrUqhqlMY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5rgmwjAqe8p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Line Banane Ki Zaroorat Kyu Padi?**  \n",
        "Agar relation **linear** hai, to iska matlab hai ke agar **X badega to Y bhi ek fixed pattern me badhega**.  \n",
        "Matlab, humein **X aur Y ke darmiyan ek straight line fit karni hogi** jo **best fit line** kehlayegi.  \n",
        "\n",
        "Ab ek **line** define karne ke liye do cheezain chahiye hoti hain:  \n",
        "1. **Slope (m)** ‚Üí Ye batata hai ke line kitni steep hai.  \n",
        "2. **Intercept (b)** ‚Üí Jab \\( x = 0 \\) ho to \\( y \\) ki value kya hogi.  \n",
        "\n",
        "Is wajah se **linear equation** banti hai:  \n",
        "\\[\n",
        "Y = mX + b\n",
        "\\]\n",
        "**Yani kisi bhi X ke liye prediction karni ho to hamein sirf m aur b ki zaroorat hai!** üî•  \n",
        "\n",
        "---\n",
        "### **Tumhara Task:**  \n",
        "Ab socho, hamein **m (slope) aur b (intercept) find karne ka koi tareeqa chahiye**.  \n",
        "Ab next step hoga **slope (m) ka formula derive karna**! üìè  \n",
        "Batao, shuru karein? üöÄ"
      ],
      "metadata": {
        "id": "WWdrhnIzqutU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQX8iLNfygew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Slope \\( m \\) Ka Formula Kaise Aya?**  \n",
        "\n",
        "Slope ka matlab hota hai **kis had tak \\( Y \\) ki value \\( X \\) ke badhnay se change hoti hai**.  \n",
        "Mathematically, slope ko define karte hain:  \n",
        "\n",
        "\\[\n",
        "m = \\frac{\\text{Change in } Y}{\\text{Change in } X}\n",
        "= \\frac{\\Delta Y}{\\Delta X}\n",
        "\\]\n",
        "\n",
        "Agar humare paas **multiple points \\( (x_1, y_1), (x_2, y_2), ... (x_n, y_n) ** hain, to humein **best slope** find karni hogi jo sabse best fit ho.\n",
        "\n",
        "#### **Derivation of Best Fit Slope Formula**  \n",
        "Humein aisa \\( m \\) chahiye jo **sabse behtareen line** de.  \n",
        "Mathematical derivation se ye formula nikalta hai:  \n",
        "\n",
        "\\[\n",
        "m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n",
        "\\]\n",
        "\n",
        "Yahan:  \n",
        "- \\( \\bar{X} \\) = **Mean of X values**  \n",
        "- \\( \\bar{Y} \\) = **Mean of Y values**  \n",
        "\n",
        "**Matlab:**  \n",
        "- **Numerator:** Ye dekhta hai ke **X aur Y ka relation kaisa hai** (covariance type).  \n",
        "- **Denominator:** Ye dekhta hai **X kitna vary kar raha hai**.  \n",
        "\n",
        "---\n",
        "### **Tumhara Task:**\n",
        "Ab socho, **m find ho gaya**! üî•  \n",
        "Lekin abhi equation incomplete hai.  \n",
        "Next step **intercept \\( b \\) ka formula derive karna hai**! üöÄ  \n",
        "Karain shuru?"
      ],
      "metadata": {
        "id": "twst-C7yyg52"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZkLXNcIKyjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8Oq1Ijg2asU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4 (Deep Understanding): Intercept \\( b \\) Kya Hai, Kyun Hai, Aur Kaise Aya?**  \n",
        "\n",
        "Hum ne **slope \\( m \\)** derive kar liya. Ab **intercept \\( b \\) ka deep breakdown** karte hain.  \n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Intercept \\( b \\) Kya Hai?**  \n",
        "Intercept \\( b \\) woh **point hai jahan line \\( Y \\)-axis ko cut karti hai**.  \n",
        "- Jab **\\( X = 0 \\)** hota hai, tab \\( Y \\) ki value \\( b \\) hoti hai.  \n",
        "- Mathematically, \\( X = 0 \\) put karne par **\\( Y = b \\) bachta hai**.  \n",
        "\n",
        "Agar tum graph dekho, agar **\\( b = 5 \\)** ho, to iska matlab hai ki line **\\( (0, 5) \\)** se guzarti hai.\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Intercept \\( b \\) Ki Zaroorat Kyu Hai?**  \n",
        "Agar sirf **slope \\( m \\)** ho aur **intercept \\( b \\) na ho**, to line sirf **origin (0,0) se pass hogi**.  \n",
        "Lekin **har dataset me aisa nahi hota**!  \n",
        "\n",
        "Agar **data origin se pass nahi hota**, to humein **intercept \\( b \\)** add karna padta hai, taki line data ke beech se jaye.\n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Intercept \\( b \\) Ka Formula Derive Karna**  \n",
        "Linear regression equation hoti hai:  \n",
        "\n",
        "\\[\n",
        "Y = mX + b\n",
        "\\]\n",
        "\n",
        "Hum **best-fit line** chahte hain jo **data ke mean point ke beech se guzray**.  \n",
        "Iska matlab hai ke **line mean values \\( (\\bar{X}, \\bar{Y}) \\) se pass hogi**.  \n",
        "\n",
        "Matlab:\n",
        "\n",
        "\\[\n",
        "\\bar{Y} = m \\bar{X} + b\n",
        "\\]\n",
        "\n",
        "Ab \\( b \\) ke liye solve karte hain:\n",
        "\n",
        "\\[\n",
        "b = \\bar{Y} - m \\bar{X}\n",
        "\\]\n",
        "\n",
        "Ye **final formula** hai jo \\( b \\) ko calculate karne ke liye use hota hai.  \n",
        "Ye formula is wajah se kaam karta hai kyunki:  \n",
        "‚úÖ **\\( \\bar{X}, \\bar{Y} \\)** mean points hain, jo data ke center ko represent karte hain.  \n",
        "‚úÖ **Best-fit line ka matlab hi ye hai ke woh mean point se guzray.**  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Example: Real Calculation**  \n",
        "Maan lo humare paas ye data hai:  \n",
        "\n",
        "| \\( X \\) | \\( Y \\)  |  \n",
        "|--------|--------|  \n",
        "| 1      | 3      |  \n",
        "| 2      | 5      |  \n",
        "| 3      | 7      |  \n",
        "\n",
        "1Ô∏è‚É£ **Calculate means:**  \n",
        "\n",
        "\\[\n",
        "\\bar{X} = \\frac{1+2+3}{3} = 2\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\bar{Y} = \\frac{3+5+7}{3} = 5\n",
        "\\]\n",
        "\n",
        "2Ô∏è‚É£ **Find slope \\( m \\)**:  \n",
        "\n",
        "\\[\n",
        "m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "m = \\frac{(1-2)(3-5) + (2-2)(5-5) + (3-2)(7-5)}{(1-2)^2 + (2-2)^2 + (3-2)^2}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "m = \\frac{(-1 \\times -2) + (0 \\times 0) + (1 \\times 2)}{(-1)^2 + (0)^2 + (1)^2}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "m = \\frac{2 + 0 + 2}{1 + 0 + 1} = \\frac{4}{2} = 2\n",
        "\\]\n",
        "\n",
        "3Ô∏è‚É£ **Find intercept \\( b \\)** using formula:  \n",
        "\n",
        "\\[\n",
        "b = \\bar{Y} - m\\bar{X}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "b = 5 - (2 \\times 2) = 5 - 4 = 1\n",
        "\\]\n",
        "\n",
        "4Ô∏è‚É£ **Final equation:**  \n",
        "\n",
        "\\[\n",
        "Y = 2X + 1\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Conclusion: Kya Seekha?**  \n",
        "- **Intercept \\( b \\) woh point hai jahan line \\( Y \\)-axis ko cut karti hai.**  \n",
        "- Agar **\\( X = 0 \\)** hota hai, to **\\( Y = b \\)** hota hai.  \n",
        "- **Intercept ko mean values ke basis par derive kiya jata hai.**  \n",
        "- **\\( b \\) ka formula \\( b = \\bar{Y} - m\\bar{X} \\) aaya hai kyunki line best-fit mean point se pass karti hai.**  \n",
        "\n",
        "**Next Step:**  \n",
        "‚úÖ **Equation mil gayi!**  \n",
        "Ab **prediction kaise hoti hai** aur **error kaise calculate karte hain**, wo dekhna hai. üöÄ"
      ],
      "metadata": {
        "id": "DpKcnNVn2bAg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ihN8lBsp2eME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPUHdOAu-vmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5 (Deep Explanation): Error Calculation ‚Äî Why & How?**  \n",
        "\n",
        "Jab hum linear regression model bana rahe hote hain, to **goal ye hota hai ke hum ek aisi line fit karein jo given data points ke sabse qareeb ho**. Lekin hamesha ek **gap hota hai model ki predictions aur actual values ke darmiyan**, jise **error** kehte hain.  \n",
        "\n",
        "Ab hum **step-by-step explore** karenge:  \n",
        "\n",
        "1. **Error ki zaroorat kyu hai?**  \n",
        "2. **Error calculate kaise hota hai?**  \n",
        "3. **Different types of error metrics**  \n",
        "4. **Error ka impact model performance pe**  \n",
        "5. **Mathematical derivation & intuition**  \n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Error Ki Zaroorat Kyu Hai?**  \n",
        "\n",
        "### **Agar error na calculate karein to kya problem hogi?**  \n",
        "Agar hum error ka calculation **na karein**, to humein ye nahi pata chalega ke:  \n",
        "- **Model sahi predict kar raha hai ya nahi?**  \n",
        "- **Koi aur better regression line ho sakti hai ya nahi?**  \n",
        "- **Model overfitting ya underfitting to nahi kar raha?**  \n",
        "\n",
        "Matlab **error ka calculation ek tarah ka score hai** jo batata hai ke model kitna accurate hai.\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Error Calculate Kaise Hota Hai?**  \n",
        "\n",
        "Jab hum kisi bhi **linear regression model** ki prediction karte hain, to humare paas **do values** hoti hain:\n",
        "\n",
        "- \\( Y_{\\text{actual}} \\) ‚Üí **Actual values** jo dataset me hain  \n",
        "- \\( Y_{\\text{predicted}} \\) ‚Üí **Predicted values** jo model se aayi hain  \n",
        "\n",
        "Error ka **basic formula**:  \n",
        "\n",
        "\\[\n",
        "Error = Y_{\\text{actual}} - Y_{\\text{predicted}}\n",
        "\\]\n",
        "\n",
        "‚úÖ Agar error **zero** hai, to iska matlab model **bilkul accurate hai.**  \n",
        "‚úÖ Agar error **positive ya negative** hai, iska matlab **model predictions aur actual values me farq hai.**  \n",
        "\n",
        "Lekin **sirf ek error ka dekhna kaafi nahi**, humein **poore dataset ka average error nikalna hota hai**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Different Types of Error Metrics**  \n",
        "\n",
        "Jab ek se zyada errors hoon, to unka **overall impact** dekhne ke liye hum **error metrics** ka use karte hain.  \n",
        "\n",
        "### **üîπ (i) Mean Absolute Error (MAE)**\n",
        "\\[\n",
        "MAE = \\frac{1}{n} \\sum |Y_{\\text{actual}} - Y_{\\text{predicted}}|\n",
        "\\]\n",
        "‚úÖ **Kya karta hai?**  \n",
        "- Ye har ek **error ka absolute value** leta hai.  \n",
        "- Phir **sabhi errors ka average** nikalta hai.  \n",
        "\n",
        "‚úÖ **Kyu use hota hai?**  \n",
        "- Absolute value lene ki wajah se **positive aur negative errors cancel nahi hote**.  \n",
        "- Simple aur **easy to interpret** metric hai.  \n",
        "\n",
        "‚úÖ **Drawback?**  \n",
        "- **Outliers ka impact nahi dikhata**, kyunki **absolute values** sirf magnitude dekhti hain.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ (ii) Mean Squared Error (MSE)**\n",
        "\\[\n",
        "MSE = \\frac{1}{n} \\sum (Y_{\\text{actual}} - Y_{\\text{predicted}})^2\n",
        "\\]\n",
        "‚úÖ **Kya karta hai?**  \n",
        "- **Har error ka square** le leta hai.  \n",
        "- Phir **sabhi squared errors ka average** nikalta hai.  \n",
        "\n",
        "‚úÖ **Kyu use hota hai?**  \n",
        "- **Large errors ka zyada impact hota hai**, kyunki **square karne se bade errors aur zyada ho jate hain**.  \n",
        "- **Continuous optimization algorithms (Gradient Descent) ke liye mathematically convenient hai**, kyunki square function **differentiable** hota hai.  \n",
        "\n",
        "‚úÖ **Drawback?**  \n",
        "- **Badi values ka zyada influence hota hai**, jo kabhi kabhi problematic ho sakta hai.  \n",
        "- **Units squared ho jati hain**, jo interpretation thodi mushkil bana deti hai.\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ (iii) Root Mean Squared Error (RMSE)**\n",
        "\\[\n",
        "RMSE = \\sqrt{MSE}\n",
        "\\]\n",
        "‚úÖ **Kya karta hai?**  \n",
        "- MSE ka **square root** le leta hai taake **error ka scale original values ke jaisa ho jaye**.  \n",
        "\n",
        "‚úÖ **Kyu use hota hai?**  \n",
        "- **Units original data jaisi hoti hain**, jo **interpretation easy banati hai**.  \n",
        "- **Bade errors ka impact zyada hota hai**, jo **model tuning me madad karta hai**.  \n",
        "\n",
        "‚úÖ **Drawback?**  \n",
        "- **Computationally thoda expensive** hota hai kyunki square root involve hota hai.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Error Ka Impact Model Performance Pe**  \n",
        "\n",
        "Agar **error zyada hai**:\n",
        "- Matlab model **accurate predictions nahi de raha**.  \n",
        "- Overfitting ya underfitting ka chance hai.  \n",
        "- Model ko **aur train karne ki zaroorat hai**.  \n",
        "\n",
        "Agar **error kam hai**:\n",
        "- Matlab model **achha kaam kar raha hai**.  \n",
        "- Predictions **actual values ke qareeb hain**.  \n",
        "- Model ko **deploy karne ke liye ready hai**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Mathematical Derivation & Intuition**  \n",
        "\n",
        "Ab hum error minimization ko **mathematically derive** karenge.  \n",
        "\n",
        "Humein ek **best-fit line** chahiye jo errors ko minimize kare.  \n",
        "Best-fit line ka equation:  \n",
        "\n",
        "\\[\n",
        "Y = mX + b\n",
        "\\]\n",
        "\n",
        "Lekin **\"best\" ka kya matlab hai?**  \n",
        "Matlab **aisi values choose karein \\( m \\) aur \\( b \\) ki, jo error ko minimize karein.**  \n",
        "\n",
        "### **Cost Function (Loss Function)**\n",
        "Har regression model **ek cost function ko optimize karta hai**, jo error ka measure hota hai.  \n",
        "Standard cost function hota hai:  \n",
        "\n",
        "\\[\n",
        "J(m, b) = \\frac{1}{2n} \\sum (Y_{\\text{actual}} - Y_{\\text{predicted}})^2\n",
        "\\]\n",
        "\n",
        "‚úÖ **Kyu square liya?**  \n",
        "- **Negative aur positive errors ko avoid karne ke liye.**  \n",
        "- **Differentiation simplify karne ke liye** (Gradient Descent me easy ho).  \n",
        "\n",
        "‚úÖ **Kya minimize karna hai?**  \n",
        "- **\\( J(m, b) \\) ko minimize karna hai**, taake best-fit line mil sake.  \n",
        "\n",
        "**Isko minimize kaise karte hain?**  \n",
        "- **Gradient Descent Algorithm ka use karte hain.**  \n",
        "- Gradient Descent iteratively **\\( m \\) aur \\( b \\) ko update karta hai**, taake cost function minimum ho jaye.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Next Step: Gradient Descent Aur Optimization**  \n",
        "‚úÖ **Ab hum dekhenge ke Gradient Descent kaise kaam karta hai** aur **kaise model ko train karta hai taake best possible line fit ho sake!** üöÄ"
      ],
      "metadata": {
        "id": "9B9UTFhd-v6r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tr6tdN57-w0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eECAIL7e-0Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Gradient Descent ‚Äì Model Training & Optimization**  \n",
        "\n",
        "Ab tak hum samajh chuke hain ke **cost function** kya hota hai aur **error ko minimize** karna kyun zaroori hai.  \n",
        "Ab **agli problem yeh hai ke hum cost function ko minimize kaise karein?** ü§î  \n",
        "\n",
        "### **üîπ Problem Statement:**\n",
        "- Humein ek **best-fit line** chahiye jo **error ko minimize kare**.  \n",
        "- **J(m, b) = (1/2n) Œ£ (Y_actual - Y_predicted)¬≤** ko minimize karna hai.  \n",
        "- Iska **best solution nikalna hai** taake hum ek **optimal regression line** bana sakein.  \n",
        "\n",
        "Lekin ek **non-trivial question ye hai**:  \n",
        "üßê **Kis tarah hum \"m\" aur \"b\" ki woh values dhundhein jo cost function ko minimum karein?**  \n",
        "\n",
        "Uska jawab hai: **Gradient Descent!** üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ What is Gradient Descent?**\n",
        "Gradient Descent ek **iterative optimization algorithm** hai jo hum **cost function ko minimize karne ke liye** use karte hain.  \n",
        "  \n",
        "### **üåç Intuition**:  \n",
        "Socho tum ek **pahari (hill) ke top par ho aur tumhe neeche utarna hai** (matlab, cost function ko minimize karna hai).  \n",
        "- Tum **seedha neeche nahi kood sakte**, warna chot lag sakti hai!  \n",
        "- Tumhe **step-by-step neeche utarna padega** taake tum **safely lowest point tak pahunch sako**.  \n",
        "\n",
        "Yahi **Gradient Descent ka kaam hai** ‚Äì har step me **thoda thoda** optimize karna aur **minimum point tak pahunchna**.  \n",
        "\n",
        "---  \n",
        "\n",
        "## **2Ô∏è‚É£ How Does Gradient Descent Work?**  \n",
        "\n",
        "### **Step 1: Compute Gradient (Slope)**\n",
        "Gradient ka matlab hota hai **slope (⁄à⁄æŸÑŸàÿßŸÜ)**.  \n",
        "Gradient batata hai ke **cost function kis direction me move kar raha hai**.  \n",
        "\n",
        "Mathematically, **gradient nikalne ke liye derivative (ŸÖÿ¥ÿ™ŸÇ) nikalte hain**:  \n",
        "\n",
        "\\[\n",
        "\\frac{\\partial J}{\\partial m} \\quad \\text{(Slope w.r.t. m)}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\frac{\\partial J}{\\partial b} \\quad \\text{(Slope w.r.t. b)}\n",
        "\\]\n",
        "\n",
        "Ye dono derivatives batayenge **kis direction me cost function tez gir raha hai**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Update Parameters (m & b)**\n",
        "Ab hum **m** aur **b** ko update karenge:  \n",
        "\n",
        "\\[\n",
        "m = m - \\alpha \\times \\frac{\\partial J}{\\partial m}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "b = b - \\alpha \\times \\frac{\\partial J}{\\partial b}\n",
        "\\]\n",
        "\n",
        "Jahan:  \n",
        "- \\( \\alpha \\) **learning rate** hai (ye control karta hai ke hum kitni tezi se move karein).  \n",
        "- \\( \\frac{\\partial J}{\\partial m} \\) aur \\( \\frac{\\partial J}{\\partial b} \\) gradients hain jo batate hain ke cost function ko kis taraf minimize karna hai.  \n",
        "\n",
        "‚úÖ **Ye step bar bar repeat hota hai jab tak cost function ka minimum na mil jaye!**  \n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Derivation of Gradient Descent for Linear Regression**  \n",
        "\n",
        "Ab hum **gradient descent ko derive karenge** aur dekhenge ke ye **kaise kaam karta hai**.\n",
        "\n",
        "Sabse pehle, hum cost function ka **partial derivative** nikalte hain.\n",
        "\n",
        "\\[\n",
        "J(m, b) = \\frac{1}{2n} \\sum (Y_{\\text{actual}} - (mX + b))^2\n",
        "\\]\n",
        "\n",
        "Agar isko **m** ke liye differentiate karein:\n",
        "\n",
        "\\[\n",
        "\\frac{\\partial J}{\\partial m} = -\\frac{1}{n} \\sum X (Y_{\\text{actual}} - Y_{\\text{predicted}})\n",
        "\\]\n",
        "\n",
        "Aur agar isko **b** ke liye differentiate karein:\n",
        "\n",
        "\\[\n",
        "\\frac{\\partial J}{\\partial b} = -\\frac{1}{n} \\sum (Y_{\\text{actual}} - Y_{\\text{predicted}})\n",
        "\\]\n",
        "\n",
        "Ye **gradients (⁄à⁄æŸÑŸàÿßŸÜ)** hain jo batayenge ke hum **kis taraf jaana hai**.\n",
        "\n",
        "Ab **m** aur **b** ko update karenge:\n",
        "\n",
        "\\[\n",
        "m = m - \\alpha \\times \\left( -\\frac{1}{n} \\sum X (Y_{\\text{actual}} - Y_{\\text{predicted}}) \\right)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "b = b - \\alpha \\times \\left( -\\frac{1}{n} \\sum (Y_{\\text{actual}} - Y_{\\text{predicted}}) \\right)\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Intuition Behind Learning Rate (Œ±)**\n",
        "**Learning Rate** ek **hyperparameter** hai jo **control karta hai ke hum kitni badi jump lein**.\n",
        "\n",
        "- **Agar Œ± bohot bara hoga** ‚Üí Gradient Descent overshoot karega aur kabhi converge nahi karega.  \n",
        "- **Agar Œ± bohot chhota hoga** ‚Üí Convergence bohot slow hoga aur training time zyada lagega.  \n",
        "- **Optimal Œ± ka selection zaroori hai taake fast aur smooth convergence ho!**  \n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Stopping Criteria (Kab Rukna Hai?)**\n",
        "Gradient Descent **tab tak chalta hai jab tak cost function ka improvement stop ho jaye**.\n",
        "\n",
        "### **Stopping Conditions:**\n",
        "1. **Change in cost function ‚âà 0**  \n",
        "   - Agar ek iteration ke baad cost function me farq bohot kam ho, to stop kar dete hain.  \n",
        "   \n",
        "2. **Fixed number of iterations**  \n",
        "   - 1000 ya 10,000 iterations tak run karna.  \n",
        "\n",
        "3. **Gradient ‚âà 0**  \n",
        "   - Jab gradient zero ke kareeb ho, iska matlab cost function ka minimum mil gaya.  \n",
        "\n",
        "---\n",
        "\n",
        "## **6Ô∏è‚É£ Types of Gradient Descent**\n",
        "1Ô∏è‚É£ **Batch Gradient Descent**  \n",
        "   - Pura dataset ek saath use hota hai.  \n",
        "   - Computationally expensive hota hai.  \n",
        "\n",
        "2Ô∏è‚É£ **Stochastic Gradient Descent (SGD)**  \n",
        "   - Har iteration me **ek random data point** use hota hai.  \n",
        "   - Fast hota hai lekin noisy updates karta hai.  \n",
        "\n",
        "3Ô∏è‚É£ **Mini-batch Gradient Descent**  \n",
        "   - Har step me **chhoti batch of samples** use hoti hai.  \n",
        "   - Balance hota hai **Batch GD** aur **SGD** ke darmiyan.  \n",
        "\n",
        "---\n",
        "\n",
        "## **7Ô∏è‚É£ Final Summary**\n",
        "‚úÖ **Gradient Descent ek iterative method hai jo cost function ko minimize karta hai.**  \n",
        "‚úÖ **Har step pe m aur b update hote hain, taake best possible line fit ho sake.**  \n",
        "‚úÖ **Learning Rate (Œ±) ka sahi selection zaroori hai, warna training fail ho sakti hai.**  \n",
        "‚úÖ **Gradient Descent different types ka hota hai: Batch, SGD, Mini-batch.**  \n",
        "‚úÖ **Stopping criteria define karni zaroori hai taake unnecessary computation na ho.**  \n",
        "\n",
        "---\n",
        "\n",
        "## **Next Step: Implementing Gradient Descent in Python! üöÄ**  \n",
        "Agar ab tak sab clear hai, to chalo **Gradient Descent ko Python me implement karte hain aur model train karte hain!** üî•"
      ],
      "metadata": {
        "id": "FmCRL4OKGyhI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1nOaf1ydGzcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}