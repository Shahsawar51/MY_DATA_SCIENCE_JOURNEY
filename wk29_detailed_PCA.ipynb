{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4mbShmIw66ERyDseQyJOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahsawar51/MY_DATA_SCIENCE_JOURNEY/blob/main/wk29_detailed_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: PCA Basics, Motivations, and Core Steps\n",
        "# What is PCA? **bold text**\n",
        "\n",
        "Definition: PCA is a statistical technique that reduces the dimensionality of a dataset while preserving most of its information (variance). It transforms data into a new coordinate system called principal components.  \n",
        "Intuition: Imagine your data as a 3D cloud of points (e.g., height, weight, age). PCA finds the directions (principal components) where the cloud is most spread out and projects the data onto those directions.  \n",
        "Example: We reduced our 3D dataset (height, weight, age) to 1D (PC1), capturing 97.96% of the variation.\n",
        "\n",
        "# **Motivations for Dimensionality Reduction**\n",
        "# **Why use PCA? Here are the key reasons: **\n",
        "\n",
        "Simplify Analysis: Fewer features make data easier to analyze and interpret (e.g., one PC1 score vs. three features).  \n",
        "Faster Machine Learning: Fewer features speed up model training (e.g., a model predicting athletic ability using PC1 trains faster).  \n",
        "Visualization: High-dimensional data (e.g., 1000D) can be plotted in 2D/1D (e.g., PC1 line plot).  \n",
        "Noise Removal: PCA discards low-variance components (e.g., PC3 with 0 variance) that may contain noise.  \n",
        "Storage Efficiency: Reduced data requires less memory (e.g., 24 numbers to 8 for our dataset).\n",
        "\n",
        "Our Case: We used PCA to simplify our 3D dataset into 1D, making it easier for analysis, visualization, and modeling.\n",
        "The Curse of Dimensionality\n",
        "\n",
        "What is it? When a dataset has too many features (e.g., 1000), it becomes sparse, causing problems for analysis and machine learning.  \n",
        "Issues:  \n",
        "Sparsity: Data points are far apart, making patterns hard to find.  \n",
        "Computation Cost: High-dimensional matrices (e.g., 1000 × 1000 covariance) are slow to process.  \n",
        "Overfitting: Models memorize noise, failing on new data.  \n",
        "Visualization: Impossible to plot 1000D data directly.\n",
        "\n",
        "\n",
        "PCA’s Role: Reduces dimensions (e.g., 1000 to 50) while keeping most info, mitigating the curse.  \n",
        "Our Case: Our dataset had only 3 features, so the curse wasn’t an issue, but PCA still simplified it effectively.\n",
        "\n",
        "PCA Steps (Theory)\n",
        "Let’s walk through PCA’s steps using our dataset:\n",
        "Step 1: Standardize the Data\n",
        "\n",
        "What? Scale features to have mean = 0 and standard deviation = 1, as features have different units (cm, kg, years).  \n",
        "How?[\\text{Standardized Value} = \\frac{\\text{Value} - \\text{Mean}}{\\text{Standard Deviation}}]  \n",
        "Our Dataset:  \n",
        "Original: Height (160-180 cm), Weight (50-70 kg), Age (15-19 years).  \n",
        "Standardized: Mean = 0, std = 1 for each.  \n",
        "Example (Student 1): Height = (\\frac{160 - 169}{6.264} \\approx -1.437).\n",
        "\n",
        "\n",
        "Why? Prevents features with larger ranges (e.g., height) from dominating.\n",
        "\n",
        "# **Step 2: Compute the Covariance Matrix**\n",
        "\n",
        "What? A matrix showing how features vary together (variance) and relate (covariance).  \n",
        "Formula:[\\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})]For standardized data (mean = 0):[\\text{Cov}(X, Y) = \\frac{1}{n} \\sum X_i \\cdot Y_i]  \n",
        "Our Case:[C = \\begin{bmatrix}1.000 & 1.000 & 0.954 \\1.000 & 1.000 & 0.954 \\0.954 & 0.954 & 1.000\\end{bmatrix}]  \n",
        "Diagonal (1.000): Variance of each feature (standardized = 1).  \n",
        "Off-diagonal (1.000, 0.954): Height-weight strongly correlated, height-age also correlated.\n",
        "\n",
        "\n",
        "Why? Captures the data’s spread and relationships.\n",
        "\n",
        "Step 3: Compute Eigenvalues and Eigenvectors\n",
        "\n",
        "What?  \n",
        "Eigenvalues ((\\lambda)): Quantify how much variance each principal component captures.  \n",
        "Eigenvectors ((v)): Define the directions of principal components.\n",
        "\n",
        "\n",
        "How? Solve the characteristic equation:[\\det(C - \\lambda I) = 0]Then find eigenvectors:[(C - \\lambda I)v = 0]  \n",
        "Our Case:  \n",
        "Polynomial: (-\\lambda^3 + 3\\lambda^2 - 0.179768\\lambda = 0).  \n",
        "Eigenvalues: (\\lambda_1 \\approx 2.9388 (97.96%)), (\\lambda_2 \\approx 0.0612 (2.04%)), (\\lambda_3 = 0 (0%)).  \n",
        "Eigenvector for (\\lambda_1): (\\begin{bmatrix} 0.577 \\ 0.577 \\ 0.577 \\end{bmatrix}) (PC1).\n",
        "\n",
        "\n",
        "Why? Eigenvalues show importance (variance), eigenvectors give the direction (new axes).\n",
        "\n",
        "Intuition: The covariance matrix is like a machine transforming data. Eigenvectors are special directions where data only stretches ((\\lambda) times), not rotates.\n",
        "\n",
        "Part 2: Projection, Evaluation, Variants, and Use Cases\n",
        "Step 4: Project Data onto Principal Components\n",
        "\n",
        "What? Map the data onto selected principal components to reduce dimensions.  \n",
        "How? Use dot product:[\\text{PC1 Score} = x \\cdot v_1 = x_1 \\cdot 0.577 + x_2 \\cdot 0.577 + x_3 \\cdot 0.577](Where (x) = standardized data point, (v_1) = PC1 eigenvector).  \n",
        "Our Case:  \n",
        "Standardized point 1: (\\begin{bmatrix} -1.437 \\ -1.437 \\ -1.234 \\end{bmatrix}).  \n",
        "PC1 score: ((-1.437 \\cdot 0.577) + (-1.437 \\cdot 0.577) + (-1.234 \\cdot 0.577) \\approx -2.370).  \n",
        "New dataset: 1D (PC1 scores) with 97.96% variance.  \n",
        "\n",
        "\n",
        "Student\n",
        "PC1\n",
        "\n",
        "\n",
        "\n",
        "1\n",
        "-2.370\n",
        "\n",
        "\n",
        "2\n",
        "-1.009\n",
        "\n",
        "\n",
        "3\n",
        "-0.089\n",
        "\n",
        "\n",
        "4\n",
        "-0.020\n",
        "\n",
        "\n",
        "5\n",
        "1.706\n",
        "\n",
        "\n",
        "6\n",
        "-2.002\n",
        "\n",
        "\n",
        "7\n",
        "3.066\n",
        "\n",
        "\n",
        "8\n",
        "0.716\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Why? Reduces dimensions (3D to 1D) while retaining most information.\n",
        "\n",
        "Step 5: Evaluate PCA Performance\n",
        "How to check if PCA worked well?  \n",
        "\n",
        "Explained Variance Ratio:  \n",
        "PC1 = 97.96%, excellent (95%+ is ideal).  \n",
        "Formula: (\\frac{\\lambda_1}{\\sum \\lambda_i}).\n",
        "\n",
        "\n",
        "Reconstruction Error:  \n",
        "Reconstruct: (\\text{PC1 Score} \\cdot v_1).  \n",
        "Mean squared error (MSE) small (e.g., point 1 error ~0.017), as only 2.04% loss.\n",
        "\n",
        "\n",
        "Downstream Task Performance:  \n",
        "ML model on PC1 (e.g., athletic prediction) has accuracy close to original data.  \n",
        "Visualization (PC1 line plot) shows clear patterns.\n",
        "\n",
        "\n",
        "Scree Plot:  \n",
        "Plot eigenvalues vs. component number. Elbow at PC1 confirms 1 component is enough.\n",
        "\n",
        "\n",
        "Cross-Validation:  \n",
        "PC1-based model’s cross-validated accuracy matches original data’s.\n",
        "\n",
        "\n",
        "\n",
        "Our Case: PCA was highly effective (97.96% variance, low error, good task performance).\n",
        "Can PCA Be Reversed?\n",
        "\n",
        "Partially Possible: Reconstruct data using PC scores and eigenvectors:[\\text{Reconstructed Point} = (\\text{PC1 Score}) \\cdot v_1]  \n",
        "Example: PC1 = -2.370 → (\\begin{bmatrix} -1.367 \\ -1.367 \\ -1.367 \\end{bmatrix}) (vs. original (\\begin{bmatrix} -1.437 \\ -1.437 \\ -1.234 \\end{bmatrix})).\n",
        "\n",
        "\n",
        "Why Not Fully? PC2 (2.04%) and PC3 (0%) info lost, as we kept only PC1.  \n",
        "Fully Possible? If all PCs (PC1, PC2, PC3) kept, 100% reconstruction, but no dimensionality reduction benefit.  \n",
        "Our Case: 2.04% loss due to PC1-only, so approximate reconstruction.\n",
        "\n",
        "PCA for Nonlinear Datasets\n",
        "\n",
        "Issue: PCA assumes linear relationships, ineffective for highly nonlinear data (e.g., weight = height²).  \n",
        "Our Case: Height, weight, age were linearly correlated (covariance: 1.000, 0.954), so PCA worked well.  \n",
        "Nonlinear Data: PCA misses curved patterns (e.g., circular data), capturing low variance.  \n",
        "Alternatives: t-SNE, UMAP, Kernel PCA for nonlinear patterns.\n",
        "\n",
        "PCA Variants and When to Use Them\n",
        "\n",
        "Vanilla PCA:  \n",
        "When? Small/medium linear datasets (e.g., our 8 rows, 3 features).  \n",
        "Why? Exact results, simple, fits in memory.\n",
        "\n",
        "\n",
        "Incremental PCA:  \n",
        "When? Large datasets (millions of rows) or streaming data that don’t fit in memory.  \n",
        "Why? Processes data in batches, memory-efficient.\n",
        "\n",
        "\n",
        "Randomized PCA:  \n",
        "When? Large, high-dimensional datasets (e.g., 1000 features, fast results needed).  \n",
        "Why? Uses approximations for speed, slight accuracy loss.\n",
        "\n",
        "\n",
        "Kernel PCA:  \n",
        "When? Nonlinear datasets (e.g., image pixels, text embeddings).  \n",
        "Why? Maps data to higher-dimensional space for nonlinear patterns, but computationally heavy.\n",
        "\n",
        "\n",
        "\n",
        "Our Case: Vanilla PCA was ideal (small, linear dataset).\n",
        "Estimating Dimensions for 95% Variance\n",
        "\n",
        "Scenario: 1000D dataset, want 95% variance.  \n",
        "How? Choose (k) PCs where:[\\frac{\\lambda_1 + \\dots + \\lambda_k}{\\sum \\lambda_i} \\geq 0.95]  \n",
        "Our Case: 1 PC (97.96%) was enough for 3D. For 1000D, typically 20-200 PCs needed, depending on data correlation.  \n",
        "Why Uncertain? Exact (k) depends on eigenvalue distribution (use scree plot).\n",
        "\n",
        "Chaining Dimensionality Reduction Algorithms\n",
        "\n",
        "Does It Make Sense? Sometimes, if algorithms complement each other.  \n",
        "When?  \n",
        "Coarse to Fine: PCA (1000D to 100D), then t-SNE (100D to 2D for visualization).  \n",
        "Linear + Nonlinear: PCA for noise removal, Kernel PCA for nonlinear patterns.  \n",
        "Efficiency: PCA simplifies data for complex methods (e.g., autoencoders).\n",
        "\n",
        "\n",
        "Our Case: Linear data, so PCA alone was enough. Chaining (e.g., PCA + t-SNE) useful if nonlinear patterns existed.  \n",
        "Risks: Info loss increases, pipeline complexity grows, redundant processing.\n",
        "\n",
        "Drawbacks of PCA\n",
        "\n",
        "Info Loss: 2.04% lost in our case (PC2). Critical if PC2 had unique info.  \n",
        "Interpretability: PCs (e.g., PC1 = height + weight + age mix) less intuitive than original features.  \n",
        "Computation Cost: Heavy for large datasets (e.g., 1000D covariance matrix).  \n",
        "Linear Assumption: Fails on nonlinear data (e.g., curved patterns).  \n",
        "Choosing PCs: Tricky to decide how many PCs to keep (scree plot helps).\n",
        "\n",
        "Final Use Cases of PCA\n",
        "\n",
        "Dimensionality Reduction: Simplified our 3D to 1D for analysis/ML.  \n",
        "Visualization: PC1 line plot showed student “size” patterns.  \n",
        "Noise Removal: Ignored PC2, PC3 (low/no variance).  \n",
        "Feature Engineering: PC1 as input for ML models (e.g., athletic prediction).  \n",
        "Compression: Reduced storage (24 to 8 numbers).\n",
        "\n",
        "Our Case: PCA created a 1D dataset (PC1) representing “overall size,” ideal for ML, visualization, and storage.\n",
        "\n",
        "Conclusion\n",
        "This notebook covered PCA’s theory in depth:  \n",
        "\n",
        "Basics: What PCA is, why it’s used, curse of dimensionality.  \n",
        "Steps: Standardization, covariance matrix, eigenvalues/eigenvectors, projection.  \n",
        "Advanced: Evaluation, reversal, nonlinear data, variants, chaining, drawbacks.  \n",
        "Our Dataset: 8 students, 3 features reduced to 1D (97.96% variance).\n"
      ],
      "metadata": {
        "id": "dCd8TbpXHCEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0U5Rjg0G82A"
      },
      "outputs": [],
      "source": []
    }
  ]
}